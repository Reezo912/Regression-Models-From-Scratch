
# Mi Propia Regresi√≥n Lineal y Log√≠stica

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.8%2B-green)](https://www.python.org/downloads/)

Este proyecto implementa algoritmos de **Regresi√≥n Lineal** y **Regresi√≥n Log√≠stica** desde cero utilizando NumPy, con el objetivo de entender los conceptos fundamentales detr√°s de estos m√©todos b√°sicos de Machine Learning.

## üéØ Descripci√≥n del Proyecto

Ambas implementaciones buscan ser educativas, demostrando:
- Fundamentos matem√°ticos claros.
- Optimizaci√≥n mediante descenso de gradiente.
- Funciones de p√©rdida y m√©tricas de evaluaci√≥n.
- Comparaci√≥n directa con implementaciones de scikit-learn.

## üìÅ Estructura del Proyecto

```
MyOwnLinearRegression/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ LinearRegression.py      # Implementaci√≥n desde cero de Regresi√≥n Lineal
‚îÇ   ‚îî‚îÄ‚îÄ LogisticRegression.py    # Implementaci√≥n desde cero de Regresi√≥n Log√≠stica
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_linear.py           # Pruebas para regresi√≥n lineal
    ‚îî‚îÄ‚îÄ test_logistic.py         # Pruebas para regresi√≥n log√≠stica
```

## üöÄ Caracter√≠sticas

### Regresi√≥n Lineal
- Regresi√≥n lineal multivariante.
- Funci√≥n de p√©rdida: Error Cuadr√°tico Medio (MSE).
- Optimizaci√≥n mediante descenso de gradiente.
- Validado con el dataset **California Housing**.

### Regresi√≥n Log√≠stica
- Clasificaci√≥n binaria.
- Funci√≥n de activaci√≥n: Sigmoide.
- Funci√≥n de p√©rdida: Log Loss (Cross-Entropy).
- Validado con el dataset **Breast Cancer Wisconsin**.

## üì¶ Instalaci√≥n

Clona el repositorio:
```bash
git clone https://github.com/Reezo912/Regression-Models-From-Scratch
cd Regression-Models-From-Scratch
```

Instala dependencias:
```bash
pip install -r requirements.txt
```

## üîß Uso R√°pido

**Nota**: Debes separar previamente tus datos en conjuntos de entrenamiento y prueba (`train_test_split`).

### Ejemplo: Regresi√≥n Lineal
```python
from src.LinearRegression import LinearRegression
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = fetch_california_housing()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Preprocesamiento
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LinearRegression(epoch=1000, lr=0.001)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

### Ejemplo: Regresi√≥n Log√≠stica
```python
from src.LogisticRegression import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Preprocesamiento
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(epoch=10000, lr=0.0001)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)
```

## üß™ Pruebas

Ejecuta scripts para comparar los modelos con scikit-learn:

```bash
python tests/test_linear.py
python tests/test_logistic.py
```

Resultados ejemplo:

```
=== SKLEARN (Logistic) ===
Accuracy: 0.9860
F1 Score: 0.9888

=== MI MODELO (Logistic) ===
Accuracy: 0.9790
F1 Score: 0.9832
```

## üìä Resultados Obtenidos

### Regresi√≥n Lineal (California Housing)
- **MSE** en conjunto de prueba comparable a scikit-learn.

### Regresi√≥n Log√≠stica (Breast Cancer Wisconsin)
- **Accuracy** superior al 97%.
- **Precision**, **Recall** y **F1-Score** similares a scikit-learn.

## üßÆ Fundamentos Matem√°ticos

| Modelo              | Hip√≥tesis                                                                                                                                     | Funci√≥n de P√©rdida                                                                                                                 |
|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|
| Regresi√≥n Lineal    | ![LinearHypothesis](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}y=Xw+b)                                                          | ![MSE](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}MSE=\frac{1}{n}\sum\left(y-\hat{y}\right)^{2})                     |
| Regresi√≥n Log√≠stica | ![LogisticHypothesis](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}y=sigmoid(Xw+b))                                               | ![LogLoss](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}LogLoss=-\frac{1}{n}\sum\left[y\log(\hat{y})+(1-y)\log(1-\hat{y})\right]) |


- **Descenso de Gradiente:** Actualizaci√≥n iterativa usando derivadas parciales.

## üõ†Ô∏è Dependencias
- `numpy`: C√°lculos num√©ricos.
- `scikit-learn`: Datasets y comparaci√≥n.

Instala todas con `pip install -r requirements.txt`.

## üîß Soluci√≥n de Problemas

### Error: "ModuleNotFoundError"
```bash
pip install -r requirements.txt
```

### Error: "ImportError"
Aseg√∫rate de estar en el directorio ra√≠z del proyecto.

### Error: "ValueError: Found input variables with inconsistent numbers of samples"
Verifica que X_train y y_train tengan el mismo n√∫mero de muestras.

### Error: "ConvergenceWarning"
Aumenta el n√∫mero de epochs o ajusta el learning rate.

## üìö Referencias y Recursos
- Libro: **Mathematics for Machine Learning** (Deisenroth & Faisal)
- Curso: **Mathematics for Machine Learning** (Imperial College London - Coursera)
- Documentaci√≥n oficial de [scikit-learn](https://scikit-learn.org/stable/)

## üöÄ Roadmap

### Pr√≥ximas Caracter√≠sticas
- [ ] Implementaci√≥n de Regularizaci√≥n (Ridge/Lasso)
- [ ] Regresi√≥n Log√≠stica Multiclase
- [ ] Validaci√≥n Cruzada
- [ ] Visualizaciones de resultados
- [ ] Optimizadores alternativos (Adam, SGD)

### Mejoras T√©cnicas
- [ ] Paralelizaci√≥n del entrenamiento
- [ ] Early stopping
- [ ] Learning rate scheduling
- [ ] Batch gradient descent

## ü§ù C√≥mo Contribuir
¬°Tu contribuci√≥n es bienvenida!
- Abre un Issue con mejoras.
- Env√≠a un Pull Request.
- Mejora documentaci√≥n y agrega ejemplos.

## üìÑ Licencia
[MIT](LICENSE)

---

**Nota Final:** Este proyecto es educativo. Para producci√≥n utiliza librer√≠as profesionales como **scikit-learn**, **TensorFlow**, o **PyTorch**.
