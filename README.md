
# Mi Propia RegresiÃ³n Lineal y LogÃ­stica

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.8%2B-green)](https://www.python.org/downloads/)

Este proyecto implementa algoritmos de **RegresiÃ³n Lineal** y **RegresiÃ³n LogÃ­stica** desde cero utilizando NumPy, con el objetivo de entender los conceptos fundamentales detrÃ¡s de estos mÃ©todos bÃ¡sicos de Machine Learning.

## ğŸ¯ DescripciÃ³n del Proyecto

Ambas implementaciones buscan ser educativas, demostrando:
- Fundamentos matemÃ¡ticos claros.
- OptimizaciÃ³n mediante descenso de gradiente.
- Funciones de pÃ©rdida y mÃ©tricas de evaluaciÃ³n.
- ComparaciÃ³n directa con implementaciones de scikit-learn.

## ğŸ“ Estructura del Proyecto

```
MyOwnLinearRegression/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ LinearRegression.py      # ImplementaciÃ³n desde cero de RegresiÃ³n Lineal
â”‚   â””â”€â”€ LogisticRegression.py    # ImplementaciÃ³n desde cero de RegresiÃ³n LogÃ­stica
â””â”€â”€ tests/
    â”œâ”€â”€ test_linear.py           # Pruebas para regresiÃ³n lineal
    â””â”€â”€ test_logistic.py         # Pruebas para regresiÃ³n logÃ­stica
```

## ğŸš€ CaracterÃ­sticas

### RegresiÃ³n Lineal
- RegresiÃ³n lineal multivariante.
- FunciÃ³n de pÃ©rdida: Error CuadrÃ¡tico Medio (MSE).
- OptimizaciÃ³n mediante descenso de gradiente.
- Validado con el dataset **California Housing**.

### RegresiÃ³n LogÃ­stica
- ClasificaciÃ³n binaria.
- FunciÃ³n de activaciÃ³n: Sigmoide.
- FunciÃ³n de pÃ©rdida: Log Loss (Cross-Entropy).
- Validado con el dataset **Breast Cancer Wisconsin**.

## ğŸ“¦ InstalaciÃ³n

Clona el repositorio:
```bash
git clone <url-del-repositorio>
cd MyOwnLinearRegression
```

Instala dependencias:
```bash
pip install -r requirements.txt
```

## ğŸ”§ Uso RÃ¡pido

**Nota**: Debes separar previamente tus datos en conjuntos de entrenamiento y prueba (`train_test_split`).

### Ejemplo: RegresiÃ³n Lineal
```python
from src.LinearRegression import LinearRegression
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

data = fetch_california_housing()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LinearRegression(epoch=1000, lr=0.001)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

### Ejemplo: RegresiÃ³n LogÃ­stica
```python
from src.LogisticRegression import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LogisticRegression(epoch=10000, lr=0.0001)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)
```

## ğŸ§ª Pruebas

Ejecuta scripts para comparar tus modelos con scikit-learn:

```bash
python tests/test_linear.py
python tests/test_logistic.py
```

Resultados ejemplo:

```
=== SKLEARN (Logistic) ===
Accuracy: 0.9860
F1 Score: 0.9888

=== MI MODELO (Logistic) ===
Accuracy: 0.9790
F1 Score: 0.9832
```

## ğŸ“Š Resultados Obtenidos

### RegresiÃ³n Lineal (California Housing)
- **MSE** en conjunto de prueba comparable a scikit-learn.

### RegresiÃ³n LogÃ­stica (Breast Cancer Wisconsin)
- **Accuracy** superior al 97%.
- **Precision**, **Recall** y **F1-Score** similares a scikit-learn.

## ğŸ§® Fundamentos MatemÃ¡ticos

| Modelo              | HipÃ³tesis                      | FunciÃ³n de PÃ©rdida                                 |
|---------------------|--------------------------------|-----------------------------------------------------|
| RegresiÃ³n Lineal    | `y = Xw + b`                   | ![MSE](https://latex.codecogs.com/svg.image?MSE=\frac{1}{n}\sum\left(y-\hat{y}\right)^{2}) |
| RegresiÃ³n LogÃ­stica | `y = sigmoid(Xw + b)`          | ![LogLoss](https://latex.codecogs.com/svg.image?LogLoss=-\frac{1}{n}\sum\left[y\log(\hat{y})\,+\,(1-y)\log(1-\hat{y})\right]) |


- **Descenso de Gradiente:** ActualizaciÃ³n iterativa usando derivadas parciales.

## ğŸ› ï¸ Dependencias
- `numpy`: CÃ¡lculos numÃ©ricos.
- `scikit-learn`: Datasets y comparaciÃ³n.

Instala todas con `pip install -r requirements.txt`.

## ğŸ“š Referencias y Recursos
- Libro: **Mathematics for Machine Learning** (Deisenroth & Faisal)
- Curso: **Mathematics for Machine Learning** (Imperial College London - Coursera)
- DocumentaciÃ³n oficial de [scikit-learn](https://scikit-learn.org/stable/)

## ğŸ¤ CÃ³mo Contribuir
Â¡Tu contribuciÃ³n es bienvenida!
- Abre un Issue con mejoras.
- EnvÃ­a un Pull Request.
- Mejora documentaciÃ³n y agrega ejemplos.

## ğŸ“„ Licencia
[MIT](LICENSE)

---

**Nota Final:** Este proyecto es educativo. Para producciÃ³n utiliza librerÃ­as profesionales como **scikit-learn**, **TensorFlow**, o **PyTorch**.
