
# Mi Propia RegresiÃ³n Lineal y LogÃ­stica

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.8%2B-green)](https://www.python.org/downloads/)
[![NumPy Version](https://img.shields.io/badge/NumPy-1.26%2B-yellow)](https://numpy.org/)

Este proyecto implementa algoritmos de **RegresiÃ³n Lineal** y **RegresiÃ³n LogÃ­stica** desde cero utilizando NumPy, con el objetivo de entender los conceptos fundamentales detrÃ¡s de estos mÃ©todos bÃ¡sicos de Machine Learning.

## ğŸ¯ DescripciÃ³n del Proyecto

Ambas implementaciones buscan ser educativas, demostrando:
- Fundamentos matemÃ¡ticos claros.
- OptimizaciÃ³n mediante descenso de gradiente.
- Funciones de pÃ©rdida y mÃ©tricas de evaluaciÃ³n.
- ComparaciÃ³n directa con implementaciones de scikit-learn.

## ğŸ“ Estructura del Proyecto

```
MyOwnLinearRegression/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ LinearRegression.py      # ImplementaciÃ³n desde cero de RegresiÃ³n Lineal
â”‚   â””â”€â”€ LogisticRegression.py    # ImplementaciÃ³n desde cero de RegresiÃ³n LogÃ­stica
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_linear.py           # Pruebas para regresiÃ³n lineal
â”‚   â””â”€â”€ test_logistic.py         # Pruebas para regresiÃ³n logÃ­stica
â””â”€â”€ scripts/
    â””â”€â”€ run_all.py               # Script para ejecutar todos los tests
```

## ğŸš€ CaracterÃ­sticas

### RegresiÃ³n Lineal
- RegresiÃ³n lineal multivariante.
- FunciÃ³n de pÃ©rdida: Error CuadrÃ¡tico Medio (MSE).
- OptimizaciÃ³n mediante descenso de gradiente.
- Validado con el dataset **California Housing**.

### RegresiÃ³n LogÃ­stica
- ClasificaciÃ³n binaria.
- FunciÃ³n de activaciÃ³n: Sigmoide.
- FunciÃ³n de pÃ©rdida: Log Loss (Cross-Entropy).
- Validado con el dataset **Breast Cancer Wisconsin**.

## ğŸ“¦ InstalaciÃ³n

Clona el repositorio:
```bash
git clone https://github.com/Reezo912/Regression-Models-From-Scratch
cd Regression-Models-From-Scratch
```

Instala dependencias:
```bash
pip install -r requirements.txt
```


## ğŸ§ª Pruebas

### EjecuciÃ³n Completa (Recomendada)
```bash
# Ejecuta todos los tests con formato profesional
python scripts/run_all.py
```

### EjecuciÃ³n Individual
```bash
# Test especÃ­fico de regresiÃ³n lineal
python tests/test_linear.py

# Test especÃ­fico de regresiÃ³n logÃ­stica  
python tests/test_logistic.py
```

## ğŸ“Š Resultados Obtenidos

### RegresiÃ³n Lineal (California Housing)
| MÃ©trica | scikit-learn | Mi ImplementaciÃ³n | Diferencia |
|---------|--------------|-------------------|------------|
| **MSE (train)** | 0.5179 | 0.5179 | 0.0000 |
| **MSE (test)** | 0.5559 | 0.5560 | +0.0001 |

âœ… **Rendimiento prÃ¡cticamente idÃ©ntico** - Diferencia de solo 0.02% en test set

### RegresiÃ³n LogÃ­stica (Breast Cancer Wisconsin)  
| MÃ©trica | scikit-learn | Mi ImplementaciÃ³n | Diferencia |
|---------|--------------|-------------------|------------|
| **Accuracy** | 98.60% | 97.90% | -0.70% |
| **Precision** | 98.89% | 98.88% | -0.01% |
| **Recall** | 98.89% | 97.78% | -1.11% |
| **F1-Score** | 98.89% | 98.32% | -0.57% |

âœ… **Excelente rendimiento** - Solo 0.7% de diferencia en accuracy con implementaciÃ³n desde cero

## ğŸ§® Fundamentos MatemÃ¡ticos

| Modelo              | HipÃ³tesis                                                                                                                                     | FunciÃ³n de PÃ©rdida                                                                                                                 |
|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|
| RegresiÃ³n Lineal    | ![LinearHypothesis](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}y=Xw+b)                                                          | ![MSE](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}MSE=\frac{1}{n}\sum\left(y-\hat{y}\right)^{2})                     |
| RegresiÃ³n LogÃ­stica | ![LogisticHypothesis](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}y=sigmoid(Xw+b))                                               | ![LogLoss](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}LogLoss=-\frac{1}{n}\sum\left[y\log(\hat{y})+(1-y)\log(1-\hat{y})\right]) |


- **Descenso de Gradiente:** ActualizaciÃ³n iterativa usando derivadas parciales.

## ğŸ› ï¸ Dependencias
- `numpy`: CÃ¡lculos numÃ©ricos.
- `scikit-learn`: Datasets y comparaciÃ³n.

Instala todas con `pip install -r requirements.txt`.

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "ModuleNotFoundError"
```bash
pip install -r requirements.txt
```

### Error: "ImportError"
AsegÃºrate de estar en el directorio raÃ­z del proyecto.

### Error: "ValueError: Found input variables with inconsistent numbers of samples"
Verifica que X_train y y_train tengan el mismo nÃºmero de muestras.

### Error: "ConvergenceWarning"
Aumenta el nÃºmero de epochs o ajusta el learning rate.

## ğŸ“š Referencias y Recursos
- Libro: **Mathematics for Machine Learning** (Deisenroth & Faisal)
- Curso: **Mathematics for Machine Learning** (Imperial College London - Coursera)
- DocumentaciÃ³n oficial de [scikit-learn](https://scikit-learn.org/stable/)

## ğŸš€ Roadmap

### ğŸ”¥ v0.0.4 - Mejoras CrÃ­ticas (En progreso)
**Objetivo**: Optimizar implementaciones actuales para mÃ¡xima robustez

**Prioridad Alta:**
- [ ] ğŸ› Arreglar consistencia de nombres (`weights` vs `pesos`)
- [ ] âš¡ Implementar parada temprana (early stopping)
- [ ] ğŸ“ˆ ProgramaciÃ³n de decaimiento de tasa de aprendizaje
- [ ] ğŸ”§ ParÃ¡metro de control de verbosidad
- [ ] ğŸ“Š Seguimiento de mÃ©tricas de convergencia
- [ ] âš ï¸ Mejores mensajes de error

**Resultado esperado**: Implementaciones perfectas y profesionales

### ğŸš€ v0.1.0 - Nuevos Algoritmos (Planificado)
**Objetivo**: Expandir familia de algoritmos de regresiÃ³n

**Nuevas Funcionalidades:**
- [ ] ğŸ¯ RegresiÃ³n Ridge (regularizaciÃ³n L2)
- [ ] ğŸ¯ RegresiÃ³n Lasso (regularizaciÃ³n L1)  
- [ ] ğŸ¯ ElasticNet (combinaciÃ³n L1 + L2)
- [ ] ğŸ“ Soporte para caracterÃ­sticas polinÃ³micas
- [ ] ğŸ”„ Marco de validaciÃ³n cruzada

### ğŸ’¡ v0.2.0 - Algoritmos Avanzados (Futuro)
**Objetivo**: Machine Learning mÃ¡s allÃ¡ de regresiÃ³n

**Algoritmos Planificados:**
- [ ] ğŸ¤– MÃ¡quinas de Vectores de Soporte (SVM)
- [ ] ğŸŒ³ Ãrboles de DecisiÃ³n desde cero
- [ ] ğŸ” Agrupamiento K-Means
- [ ] ğŸ“Š AnÃ¡lisis de Componentes Principales (PCA)
- [ ] ğŸ§  Red Neuronal (PerceptrÃ³n)

### ğŸ› ï¸ Mejoras de Infraestructura (Continuas)
- [ ] ğŸ“ˆ Suite de evaluaciÃ³n de rendimiento
- [ ] ğŸ¨ Herramientas de visualizaciÃ³n de datos
- [ ] ğŸ“¦ Pruebas automatizadas con GitHub Actions
- [ ] ğŸ“š DocumentaciÃ³n de API con Sphinx
- [ ] ğŸ³ ContenedorizaciÃ³n con Docker

## ğŸ¤ CÃ³mo Contribuir
Â¡Tu contribuciÃ³n es bienvenida!
- Abre un Issue con mejoras.
- EnvÃ­a un Pull Request.
- Mejora documentaciÃ³n y agrega ejemplos.

## ğŸ“„ Licencia
[MIT](LICENSE)

---

**Nota Final:** Este proyecto es educativo. Para producciÃ³n utiliza librerÃ­as profesionales como **scikit-learn**, **TensorFlow**, o **PyTorch**.
